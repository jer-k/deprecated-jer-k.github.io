---
published: false
---
# Introduction
Summarize Configurator. Show Image of all the steps. Describe that it took a week or more, many manual repeated steps.

# VIPR
Given all that, let’s talk about VIPR, which stands for Vehicle Intelligence Project Resource. My guess is that the name came from a desire to be a car, but sometimes the acronym isn’t perfect, but the repo was created before I joined the project. We’re going to cover a lot of ground very quickly.


As we set out to build VIPR the primary objective was to get away from a process that took multiple days and build an application that could load and publish data every day. Because this is the core of the application, we’re going to take some time to look at how we accomplish our data ingestion process and how that builds the foundation of which all the other data sets rely on before covering some, but not all, of the other functionality of the application and where you can find that data on our website today.

# Topics to Cover
Data Ingestion Process
Data Editing UI
Publish UI
Features UI
Images UI


# VIPR Data Ingestion Process
A foreword that this section is going to be the only section that contains code references and is a deep dive into the behind the scenes process.
At the core of VIPR is a daily ingestion process of Chrome data. They provide us with CSV files that have their own schema, but we only take parts of that schema and transform it into our own. For each file we have a loader class which defines the schema from the file, along with other meta data points and functions that can run after we insert rows into table. Here is an example loader

```ruby
raw_table_name 'makes'
raw_key 'makeId'
raw_schema(
  'makeId' => 'integer',
  'manufacturerId' => 'integer',
  'makeName' => 'varchar(50)'
)

file_name 'Makes'
model Make
key :external_make_id

mappings(
  external_make_id: 'makeId',
  name: 'makeName'
)

associations(
  Manufacturer => { external_manufacturer_id: 'manufacturerId' }
)

before_load do
end

after_load do
end

before_create do |make, row|
end

after_create do |make_attrs, raw_row|
end
```

We’ll cover each piece briefly. 
`raw_table_name` simply defines the name of the table on our raw PostgresSQL schema. 
`raw_key` indicates the primary key of that table
`raw_schema` is the schema definition of the table
`file_name` tells the loader which file to read the data from and model is accompanying ActiveRecord based class. 
`mappings` indicate how we’re mapping the data from the raw table to the ActiveRecord based table
`associations` tells us how to find the associated classes based on its own mapping.
`before_load` runs before the loads starts doing any processing
`after_load` is the final block the loader will run
`before_create` allows for preprocessing of any row before it goes into the database
`after_create` allows for postprocessing

Now that we have an idea about how we’re translating the data, lets quickly dive into how we load the data every day. First lets take a look at parts of a report that is generated if we were to initialize our local environment with the full data set.

```ruby
Slurping raw files into `raw_schema_20190530170648`
Manufacturers            |      1.0s taken           810 records slurped     772.7 records/sec
Makes                    |      0.8s taken         1,014 records slurped
Models                   |      0.9s taken         8,040 records slurped
FeatureTypes             |      1.0s taken           792 records slurped     772.9 records/sec
Features                 |      0.8s taken         5,861 records slurped
SpecificationTypes       |      1.0s taken         6,852 records slurped    6723.2 records/sec
Styles                   |      1.2s taken        65,609 records slurped   55652.5 records/sec
VinPatterns              |      5.8s taken       283,424 records slurped   48586.4 records/sec
VehicleStandards         |     69.9s taken     5,109,844 records slurped   73063.2 records/sec
Specifications           |     58.4s taken     8,140,581 records slurped  139387.2 records/sec
Options                  |     55.6s taken     3,191,725 records slurped   57408.6 records/sec
OptionRules              |     22.9s taken     3,054,255 records slurped  133460.3 records/sec
Prices                   |     22.9s taken     3,280,782 records slurped  143130.2 records/sec
Colors                   |    191.8s taken     3,381,339 records slurped   17628.0 records/sec

Importing raw tables into VIPR schema
Manufacturers            |      1.1s taken            41 records      36.9 records/sec
Makes                    |      1.1s taken            62 records      56.4 records/sec
Models                   |    194.2s taken         8,040 records      41.4 records/sec
FeatureTypes             |      0.1s taken            33 records
Features                 |      0.1s taken           245 records
SpecificationTypes       |      0.2s taken           290 records
Styles                   |   2768.1s taken        65,609 records      23.7 records/sec
VinPatterns              |   3034.5s taken       225,832 records      74.4 records/sec
VehicleStandards         |   1256.4s taken     9,469,241 records    7536.7 records/sec
Specifications           |   1818.4s taken     8,066,938 records    4436.3 records/sec
Options                  |   1383.9s taken     7,341,909 records    5305.2 records/sec
OptionRules              |    762.8s taken     2,925,422 records    3835.1 records/sec
Prices                   |    970.4s taken     3,280,782 records    3380.8 records/sec
Colors                   |   1895.7s taken     1,576,469 records     831.6 records/sec
```

As you can see that’s roughly XX million rows that had to be inserted (XX raw rows and YY vipr rows), along with extra processing from tracking and creating all the associations and executing all the callback functions. I’m hand waving over the full code that actually runs the entire process, but it’s a Ruby class that batches up the data to insert and tracks what is inserted via a class level cache so we can build the associations. Instead we’ll cover parts of the process in pseudo code and link to some real code on Github. To do heavy inserting, we rely on [PostgreSQL COPY](https://www.postgresql.org/docs/current/sql-copy.html). If you aren’t familiar with COPY, it is described as


COPY moves data between PostgreSQL tables and standard file-system files. COPY TO copies the contents of a table to a file, while COPY FROM copies data from a file to a table (appending the data to whatever is in the table already). COPY TO can also copy the results of a SELECT query.

Here’s how we would go about initially loading the data
- Create a new PostgresSQL schema raw_data_
- Loop through each loader class creating a table in the raw_data schema
- Insert the raw data from the csv files into the raw tables via our copy_from_file function
- Loop through each loader class again, taking the raw data in batches using a [CURSOR](https://www.postgresql.org/docs/current/plpgsql-cursors.html) and executing the callback methods while building up the id cache using insert_with_copy.

Now we have a database that has been initialized on a given day (recall the schema we created raw_data_). However, vehicle data is an ever moving dataset and not only are we getting new vehicles, but we’re also receiving updates about previous vehicles. We needed to build upon our initial loading process to determine what updates occur each day. Let’s take a look at the output of what our daily loading process looks like.

  
```ruby
Removing old daily raw loads
Removed raw_schema_20190526100001
Starting daily load into schema `raw_schema_20190529100000`
Slurping raw files into `raw_schema_20190529100000`
... same slurping logs as the initial load above ...

Finding differences between `raw_schema_20190529100000` and `raw_schema_20190528100000`
Raw change counts...
Manufacturers            |      0.0s taken           0 differences           0  objects
Divisions                |      0.0s taken           0 differences           0  objects
Models                   |      0.1s taken          12 differences           7  objects
CategoryHeaders          |      0.0s taken           0 differences           0  objects
Categories               |      0.0s taken           0 differences           0  objects
StandardHeaders          |      0.0s taken           0 differences           0  objects
TechTitleHeaders         |      0.0s taken           0 differences           0  objects
TechTitles               |      0.0s taken           0 differences           0  objects
Styles                   |      3.8s taken         172 differences         126  objects
VinPatterns              |      5.1s taken         462 differences         462  objects
Standards                |    156.1s taken      12,121 differences       5,474  objects
TechSpecs                |    157.4s taken      16,909 differences      10,389  objects
Options                  |    178.9s taken      19,267 differences       8,747  objects
OrderRules               |     81.7s taken      41,261 differences      41,261  objects
Prices                   |     91.2s taken      33,166 differences       9,852  objects
Colors                   |     57.2s taken         774 differences         680  objects

Daily load finished in 24 minutes (1462.0s)
Auto accepting changes... (✔ = auto-accepted, • = pending, ✘ = failed)
Manufacturers            | ✔        0 •        0 ✘        0
Divisions                | ✔        0 •        0 ✘        0
Models                   | ✔        0 •        7 ✘        0
CategoryHeaders          | ✔        0 •        0 ✘        0
Categories               | ✔        0 •        0 ✘        0
StandardHeaders          | ✔        0 •        0 ✘        0
TechTitleHeaders         | ✔        0 •        0 ✘        0
TechTitles               | ✔        0 •        0 ✘        0
Styles                   | ✔        0 •       74 ✘        0
VinPatterns              | ✔        0 •        0 ✘      462
Standards                | ✔      540 •        0 ✘        0
TechSpecs                | ✔      400 •        0 ✘        0
Options                  | ✔    3,768 •        0 ✘        0
OrderRules               | ✔   35,332 •        0 ✘    5,929
Prices                   | ✔    4,628 •        0 ✘        0
Colors                   | ✔       45 •        0 ✘        9
```

Each day we build another new schema, raw_data_ and repeat the steps of loading data into the raw tables. This time, instead of looping through the loaders again and mapping the data into our schema, we run a query that returns a diff between today’s raw tables () and yesterday’s raw tables (). The diffs create three types ‘changes’: ‘create’, ‘update’, and ‘destroy’. A ‘create’ change surfaces when we find a value for the `raw_key` attribute in today’s schema that isn’t there in yesterday. The opposite is true for a destroy, where in a `raw_key` is not present in today’s table, but was there yesterday. Finally an `update` change is created when the `raw_key` is present in both tables and we notice that the value of one or more columns has changed. The result of the query is looped through and turned into Ruby hash’s and then inserted into the database again via `insert_with_copy`. This process runs every night so that the changes ready to be reviewed by the data team in the morning.

Here is an example of a ‘create’ change returned from the database.

That team is able to accept the changes, which in the case of our create example above, would create a new record on the styles table or reject them, if say an ‘update’ change seems to change a value to something incorrect. 


###
Each section summarizes how we can use Vipr to acheive better things. Show a picture/gif of managing the data in Vipr and a picture + link of where it exists on TCDC

# Data Editing UI
If you recall from the first section, we covered the topic of having to repeat manual entry of data into CSV’s each week during the Configurator process. In VIPR, which all the data always residing in the database, there is no longer any repeat work. Once a data team member adds or edits data, it gets stored in the database and hopefully never has be modified again. Let’s take a look at the one of the main UI’s for this data editing.

Whoops, I think we ended up rebuilding Excel in a web app… but in the end editing the data in Excel is what many of the data team members felt comfortable with. Not all functionality of Excel is present, but let’s run through a few things we can do.

Add multiple columns of data

Reorganize the columns

Add changes and see highlights of what has actually changed

Apply changes across multiple columns

Finally, we can save the changes to the database.

# Publishing UI

###
Describe how we can publish data to different environments and how that ties into spacepods.
  
# Generic Features UI
Another goal that arose during the time we were building VIPR was to have the ability to create generic features. What I mean by that is if you look at BMW 3 Series and you want to add <> it is called <>, but if you look at this Merdecez-Benz C Class it is called <>. The problem this creates is when a car buyer wants to cross shop these vehicles, without knowing this exact nomenclature defined by the OEM’s, it is hard to understand if both vehicles have <>. This is our where VIPR comes in. We 

# Images UI

###
Refer back to Configurator how they had to manually repeat steps to manage images. Show the UI how we resize things and such

# Outro
Its really cool
